<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学习笔记(1)——线性回归</title>
      <link href="/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
      <url>/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h3 id="1-常见分类："><a href="#1-常见分类：" class="headerlink" title="1 常见分类："></a><strong>1 常见分类：</strong></h3><p>常见的机器学习分为监督学习和无监督学习。</p><ul><li>监督学习是根据已有数据集，已知输入和输出结果之间的关系，然后根据这种已知关系训练得到一个最优模型。</li><li>在无监督学习中，我们需要用某种算法去训练无标签的训练集从而能让我们我们找到这组数据的潜在结构，而不需告知数据的内涵和意义。</li></ul><h3 id="2-一元线性回归模型描述："><a href="#2-一元线性回归模型描述：" class="headerlink" title="2 一元线性回归模型描述："></a><strong>2 一元线性回归模型描述：</strong></h3><ul><li>$m$ ：训练样本的数量</li><li>$x$ ：输入变量（自变量、特征）</li><li>$y$ ：目标变量</li><li>$ (x^{(i)}, y^{(i)}) $ ：表示第 $i$ 个训练样本</li><li>$h_\theta(x)=\theta_0+\theta_1x$ ：假设函数 </li></ul><h3 id="3-代价函数："><a href="#3-代价函数：" class="headerlink" title="3 代价函数："></a><strong>3 代价函数：</strong></h3><p>以线性回归为例，对于假设函数 $h_\theta(x)=\theta_0+\theta_1x$ ，其目标是，寻找出一个最合理的参数集合${ \theta_0, \theta_1 }$，使得假设函数对于目标变量的误差最小。因此，一元线性回归可以简化为以下问题：</p><script type="math/tex; mode=display">\underset{\theta _0\ \ \ \ \ \theta _1}{\min\text{imize}}\,\,\frac{1}{2m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}</script><p>其中，选取其误差和的平均值的 $\frac{1}{2}$ 是为了对其求导时，系数变为1，研究方便。并无实际影响。</p><p>对此，我们将 </p><script type="math/tex; mode=display">J\left( \theta _0,\theta _1 \right) =\,\,\frac{1}{2m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}</script><p>称做为<strong>代价函数</strong>。</p><p>解决问题，就是解决求出使得代价函数取得最小值的 ${ \theta_0, \theta_1 }$ 问题。</p><h3 id="4-正规方程法："><a href="#4-正规方程法：" class="headerlink" title="4 正规方程法："></a><strong>4 正规方程法：</strong></h3><p>对于线性回归，不管是一元还是多元，对于假设函数$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+…+\theta_nx_n=\theta^Tx$ ，通常，约定为$x_0^{(i)}$恒为一。构造矩阵</p><script type="math/tex; mode=display">X=\left[ \begin{array}{c}    x^{\left( 1 \right) T}\\    x^{\left( 2 \right) T}\\    ...\\    x^{\left( m \right) T}\\\end{array} \right] ;\ \theta =\left[ \begin{array}{c}    \theta _0\\    \theta _1\\    ...\\    \theta _n\\\end{array} \right] ;\ y=\left[ \begin{array}{c}    y_1\\    y_2\\    ...\\    y_m\\\end{array} \right] .</script><p>我们的目标是找到一个$\theta$，它的值尽可能满足$y=X\theta$，因此，$X^Ty=X^TX\theta$ ;$\theta={(X^TX)}^{-1}X^Ty$ 。也就是说，我们可以直接求出参数$\theta$的值，这种方法被称为正规方程法。<strong>但是当$m&lt;n$时，训练样本数小于特征数，会导致$(X^TX)$是奇异矩阵，此时再计算${(X^TX)}^{-1}$时会出现错误，这一点将在后面的正则化中解决。</strong></p><h3 id="5-梯度下降："><a href="#5-梯度下降：" class="headerlink" title="5 梯度下降："></a><strong>5 梯度下降：</strong></h3><p>梯度下降算法是有效求解上述问题的算法。对于适当的问题，它总能收敛于局部最优解或全局最优解。当然，与之相对应的还有梯度上升算法，分别用来求极小值和极大值。</p><h4 id="5-1-梯度下降算法的更新方程"><a href="#5-1-梯度下降算法的更新方程" class="headerlink" title="5.1 梯度下降算法的更新方程"></a><strong>5.1 梯度下降算法的更新方程</strong></h4><p>仍然以一元线性回归的假设函数举例，则其梯度下降算法的更新方程为：</p><script type="math/tex; mode=display">\theta _j=\theta _j-\alpha \frac{\partial}{\partial \theta _j}J\left( \theta _0,\theta _1 \right) .\ \ \ \ \ \ \ \ \ \left( for\ j=0\ and\ j=1 \right)</script><p>其中，公式中参数有一下含义：</p><ul><li>$\alpha$ ：学习速率常数，一定程度上控制着梯度下降的速率</li><li>$\theta_j$ ：回归分析中的参数</li></ul><p>梯度下降算法更新方程利用了导数在极值点两侧的异号性和曲线在极值点附近的速率变缓性，接近局部最低时，梯度下降能够利用更新方程自动调节步长，最终使参数收敛于局部最优点。</p><p>对于参数 $\alpha$ ，它一定程度上决定了两次下降之间的步长，因此， $\alpha$ 取值不当有可能造成一些不好的结果。</p><p><strong>当学习速率 $\alpha$ 过小时，会使得学习过程较慢，步数较多；当学习速率过大时，可能导致结果越过最低点，造成无法收敛，甚至发散等问题。</strong></p><h4 id="5-2-梯度下降算法注意事项"><a href="#5-2-梯度下降算法注意事项" class="headerlink" title="5.2 梯度下降算法注意事项"></a><strong>5.2 梯度下降算法注意事项</strong></h4><h5 id="同步更新次序："><a href="#同步更新次序：" class="headerlink" title="同步更新次序："></a>同步更新次序：</h5><p>以一元线性回归为例，$\theta_0 , \theta_1$ 发生更新时，要保证其同时更新，避免先更新的一方参与另一方的更新公式运算，造成更新混乱。</p><p>如：</p><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">// J(theta_0,theta_1) 代价函数</span><br><span class="line">// PartialDerivative(theta_0,J) 对代价函数J求偏导</span><br><span class="line"></span><br><span class="line">// Correct</span><br><span class="line">//同步更新</span><br><span class="line">temp0 = theta_0 - alpha * PartialDerivative(theta_0,J);</span><br><span class="line">temp1 = theta_1 - alpha * PartialDerivative(theta_1,J);</span><br><span class="line">theta_0 = temp0;</span><br><span class="line">theta_1 = temp1;</span><br><span class="line"></span><br><span class="line">// Incorrect</span><br><span class="line">//没有同步更新</span><br><span class="line">temp0 = theta_0 - alpha * PartialDerivative(theta_0,J);</span><br><span class="line">theta_0 = temp0;</span><br><span class="line">temp1 = theta_1 - alpha * PartialDerivative(theta_1,J);</span><br><span class="line">theta_1 = temp1;</span><br></pre></td></tr></table></figure><h3 id="6-一元线性回归的梯度下降："><a href="#6-一元线性回归的梯度下降：" class="headerlink" title="6 一元线性回归的梯度下降："></a><strong>6 一元线性回归的梯度下降：</strong></h3><h4 id="6-1-更新公式"><a href="#6-1-更新公式" class="headerlink" title="6.1 更新公式"></a><strong>6.1 更新公式</strong></h4><p>一元线性回归的代价函数为: $J\left( \theta <em>0,\theta _1 \right) =\,\,\frac{1}{2m}\sum</em>{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}$ ，对其求偏导，可以求得：</p><script type="math/tex; mode=display">\left\{ \begin{array}{l}    \begin{array}{c}    \frac{\partial}{\partial \theta _0}J\left( \theta _0,\theta _1 \right) =\frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right)}\\\end{array}\\    \,\,\frac{\partial}{\partial \theta _1}J\left( \theta _0,\theta _1 \right) =\frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ·x^{\left( i \right)}}\\\end{array} \right.</script><h4 id="6-2-凸函数"><a href="#6-2-凸函数" class="headerlink" title="6.2 凸函数"></a><strong>6.2 凸函数</strong></h4><p>一般来说，在处理一个问题时，将具有全局最优解的函数称为凸函数，在解决这类问题时，也称之为凸优化。在算法的应用中，应尽可能的保证代价函数是凸函数，这样可以稳定的使结果收敛与全局最优解而非局部最优解。</p><h3 id="7-多元梯度下降法："><a href="#7-多元梯度下降法：" class="headerlink" title="7 多元梯度下降法："></a><strong>7 多元梯度下降法：</strong></h3><p>上面讨论了一元线性回归的梯度下降法，那么多元梯度下降法该怎么表示并实现呢？</p><p>对于多元假设函数 $h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+…+\theta_nx_n=\theta^Tx$ ，这里的$\theta$和$x$分别是一个$n+1$维的向量。仍然利用上面讲过的梯度下降算法，那么多元假设函数的变量更新方程为：</p><script type="math/tex; mode=display">\theta _0=\theta _0-\alpha \frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ·1}\\\theta _1=\theta _1-\alpha \frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ·x_1^{\left( i \right)}}\\\theta _2=\theta _2-\alpha \frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ·x_2^{\left( i \right)}}\\...\\\theta _n=\theta _n-\alpha \frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ·x_n^{\left( i \right)}}</script><p>如果将$x_0^{(i)}$约定为1，上述$n+1$个式子就可以统一起来，只需一个循环，就可以将所有$n+1$个参数进行更新。<strong>但由于每次更新时，要求同步更新，因此，循环实现同步更新较为复杂。因此，需要找到一种可以直接同步更新的方法，通过引入矩阵，就可以较快速的完成同步更新。</strong></p><p>对于上面的参数更新方程，想要同步更新，则对于向量$\theta,X,y$，要将上述式子用向量化的形式表示出来。对于$\left( h<em>{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) $项，显然可以发现，用$X\theta-y$就可以表示出全部$m$个样本的误差，因此，$\left( h</em>{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) =\left( X\theta-y \right) \epsilon \mathbb{R}^m$。</p><p>而对于每个$x<em>j^{\left( i \right)}(i=1…m)$，其都要与$X\theta-y$中的每一项相乘并结果相加才能得到$\sum</em>{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ·x_j^{\left( i \right)}}$，可以很自然的想到，这一求和项的值最终等于一个$1\times m$的矩阵和$X\theta-y$即$m\times 1$的矩阵相乘得到。事实上，这个所说的$1\times m$矩阵，就是$X^T$的第$j$行。</p><p>因此，我们得到了向量形式的更新方程：$\theta =\theta -\alpha \frac{1}{m}X^T\left( X\theta -y \right) $。通过矩阵，可以快速的更新整个$\theta$向量。解决了多元线性回归参数难以同步更新的问题。</p><h3 id="8-过拟合问题，正则化"><a href="#8-过拟合问题，正则化" class="headerlink" title="8 过拟合问题，正则化"></a><strong>8 过拟合问题，正则化</strong></h3><p>如下图所示，对于下列样本数据，我们可以使用A,B,C三种不同的线性回归方式，A是一次线性回归，B中加入了高次项，使得回归效果更好。C中加入了更多的高次项，然而其回归效果并不是很好，其参数值受样本点的波动较大。<strong>这种现象称之为“过拟合“</strong>。</p><center class="half"><img src="/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/3.png"></center><p>为了解决过拟合问题，有很多可以使用的方法，比较常用的是正则化方法。对于一些特征，也许加入这些特征会产生过拟合问题，但是仍然无法抛弃这些特征时，可以通过正则化的方法使得过拟合问题减弱，曲线仍然是比较平滑的。正则化方法即在原有代价函数的基础上，加入参数的惩罚项，来减弱参数值，以此来减弱某些参数对结果造成过拟合问题。</p><p>正则化的思想是加入惩罚项，使得参数$\theta$的值变得更小，从而削弱某些特征的参数过大产生拟合问题的可能性。</p><h4 id="8-1梯度下降的正则化"><a href="#8-1梯度下降的正则化" class="headerlink" title="8.1梯度下降的正则化"></a><strong>8.1梯度下降的正则化</strong></h4><p>在上文中提到了线性回归的代价函数$J\left( \theta\right) =\,\,\frac{1}{2m}\sum<em>{i=1}^m{\left( h</em>{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}$。在正则化中，我们将对代价函数$J(\theta)$进行修正，因为不知道是哪项参数导致过拟合问题，我们在加入惩罚项时，直接考虑到所有的参数（但不包括$\theta_0$，因为常数项不会引起过拟合问题），修正后的代价函数如下：</p><script type="math/tex; mode=display">J\left( \theta\right) =\,\,\frac{1}{2m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2+\lambda\sum_{j=1}^n{\theta^2_j}}</script><p>同样利用上面讲过的梯度下降算法，那么多元假设函数的变量更新方程也发生了修正，修正后为：</p><script type="math/tex; mode=display">\theta _0=\theta _0-\alpha \frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ·x_0^{\left( i \right)}}\\\theta _1=\theta _1-\alpha \left[ \frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ·x_1^{\left( i \right)}+\frac{\lambda}{m}\theta_1}\right]\\\theta _2=\theta _2-\alpha \left[\frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ·x_2^{\left( i \right)}+\frac{\lambda}{m}\theta_2}\right]\\...\\\theta _n=\theta _n-\alpha \left[ \frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ·x_n^{\left( i \right)}+\frac{\lambda}{m}\theta_n}\right]</script><p>同样的，正则化后的矩阵化参数更新方程也发生了改变，对于$\theta$，其更新方程变为</p><script type="math/tex; mode=display">\theta =\left(\theta-\alpha\frac{\lambda}{m}\left[ \begin{array}{c}    0\\    1\\    ...\\    1\\\end{array} \right] \theta\right) -\alpha \frac{1}{m}X^T\left( X\theta -y \right)</script><p>。</p><h4 id="8-2正规方程的正则化"><a href="#8-2正规方程的正则化" class="headerlink" title="8.2正规方程的正则化"></a><strong>8.2正规方程的正则化</strong></h4><p>对于正规方程法，我们上面提到$\theta={(X^TX)}^{-1}X^Ty$，存在着可能会出现奇异矩阵的问题的缺点。我们也可以对其进行正则化修正。修正后结果为：$\theta={(X^TX+\lambda\left[ \begin{matrix}<br>    0&amp;        &amp;        &amp;        \<br>    &amp;        1&amp;        &amp;        \<br>    &amp;        &amp;        …&amp;        \<br>    &amp;        &amp;        &amp;        1\<br>\end{matrix} \right] )}^{-1}X^Ty$。可以保证的是，$\left(X^TX+\lambda\left[ \begin{matrix}<br>    0&amp;        &amp;        &amp;        \<br>    &amp;        1&amp;        &amp;        \<br>    &amp;        &amp;        …&amp;        \<br>    &amp;        &amp;        &amp;        1\<br>\end{matrix} \right]\right)$ 一定是一个可逆矩阵，因此，正则化后，祛除了其可能出现奇异矩阵的缺点。</p><h3 id="9-练习"><a href="#9-练习" class="headerlink" title="9 练习"></a><strong>9 练习</strong></h3><p>一家保险公司十分关心其总公司营业部加班的程度，决定认真调查一下现状。经10周时间，收集了每周加班数据和签发的新保单数目，x为每周签发的新保单数目，y为每周加班时间(小时)，数据如表所示：</p><div class="table-container"><table><thead><tr><th style="text-align:center">周序号</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th><th style="text-align:center">6</th><th style="text-align:center">7</th><th style="text-align:center">8</th><th style="text-align:center">9</th><th style="text-align:center">10</th></tr></thead><tbody><tr><td style="text-align:center">X</td><td style="text-align:center">825</td><td style="text-align:center">215</td><td style="text-align:center">1070</td><td style="text-align:center">550</td><td style="text-align:center">480</td><td style="text-align:center">920</td><td style="text-align:center">1350</td><td style="text-align:center">325</td><td style="text-align:center">670</td><td style="text-align:center">1215</td></tr><tr><td style="text-align:center">Y</td><td style="text-align:center">3.5</td><td style="text-align:center">1</td><td style="text-align:center">4</td><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">3</td><td style="text-align:center">4.5</td><td style="text-align:center">1.5</td><td style="text-align:center">3</td><td style="text-align:center">5</td></tr></tbody></table></div><p>首先，绘制出关于x,y的散点图如下：</p><center class="half">    <img src="/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.png" width="800" style="zoom: 80%;"></center><p>可以看出，x与y之间近似服从线性关系，且属于一元线性回归。因此，采用梯度下降算法对参数进行回归求解，其MATLAB代码如下：</p><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">x=[<span class="number">825</span>,<span class="number">215</span>,<span class="number">1070</span>,<span class="number">550</span>,<span class="number">480</span>,<span class="number">920</span>,<span class="number">1350</span>,<span class="number">325</span>,<span class="number">670</span>,<span class="number">1215</span>]&#x27;;</span><br><span class="line">y=[<span class="number">3.5</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4.5</span>,<span class="number">1.5</span>,<span class="number">3</span>,<span class="number">5</span>]&#x27;;</span><br><span class="line"><span class="comment">%plot(x,y,&#x27;r*&#x27;);</span></span><br><span class="line">x=(x-<span class="built_in">min</span>(x))/(<span class="built_in">max</span>(x)-<span class="built_in">min</span>(x));<span class="comment">%对x进行特征缩放</span></span><br><span class="line">y=(y-<span class="built_in">min</span>(y))/(<span class="built_in">max</span>(y)-<span class="built_in">min</span>(y));<span class="comment">%对y进行特征缩放</span></span><br><span class="line">X=[<span class="built_in">ones</span>(<span class="built_in">length</span>(x),<span class="number">1</span>),x];<span class="comment">%构建X矩阵</span></span><br><span class="line">n=<span class="number">2</span>;<span class="comment">%参数个数</span></span><br><span class="line">k=<span class="number">10000</span>;<span class="comment">%取10000次迭代</span></span><br><span class="line">alpha=<span class="number">0.1</span>;</span><br><span class="line">lambda=<span class="number">0</span>;<span class="comment">%这里不需要正则化</span></span><br><span class="line">m=<span class="built_in">length</span>(x);</span><br><span class="line">theta=<span class="built_in">ones</span>(<span class="number">2</span>,<span class="number">1</span>);<span class="comment">%对参数赋初始值</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:k</span><br><span class="line">    theta=(theta-alpha*lambda/m*[<span class="number">0</span>;<span class="built_in">ones</span>(n<span class="number">-1</span>,<span class="number">1</span>)])-alpha*<span class="number">1</span>/m*X&#x27;*(X*theta-y);<span class="comment">%更新方程</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">x2=<span class="number">0</span>:<span class="number">100</span>:<span class="number">1600</span>;</span><br><span class="line">x2=(x2-<span class="built_in">min</span>(x2))/(<span class="built_in">max</span>(x2)-<span class="built_in">min</span>(x2));</span><br><span class="line">X2=[<span class="built_in">ones</span>(<span class="built_in">length</span>(x2),<span class="number">1</span>),x2&#x27;];<span class="comment">%验证</span></span><br><span class="line"><span class="built_in">plot</span>(x2,X2*theta,<span class="string">&#x27;b-&#x27;</span>);</span><br><span class="line"><span class="built_in">hold</span> on;</span><br><span class="line"><span class="built_in">plot</span>(x,y,<span class="string">&#x27;r*&#x27;</span>);</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>结果如下：</p><center class="half">    <img src="/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/2.png" width="800" style="zoom: 80%;"></center><p>直观的可以看出，线性回归的拟合较为成功。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性回归 </tag>
            
            <tag> 代价函数 </tag>
            
            <tag> 梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记(1)——logistic回归</title>
      <link href="/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(2)%E2%80%94%E2%80%94logistic%E5%9B%9E%E5%BD%92/"/>
      <url>/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(2)%E2%80%94%E2%80%94logistic%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h3 id="1-logistic回归"><a href="#1-logistic回归" class="headerlink" title="1 logistic回归"></a><strong>1 logistic回归</strong></h3><p>&emsp;&emsp; logistic回归是一种分类算法，虽然其名称中含有“回归”两个字，其实本质上他是一种分类算法。</p><p>&emsp;&emsp;所谓分类算法，通俗的说，如下图所示，就是对于两种状态（0或者1），即下图中的红色点和绿色点，能够找出一条线将两种状态分隔开来。在机器学习中，logistic回归是一个非常重要的概念，也是一个很好的算法。</p><p><img src="/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(2)%E2%80%94%E2%80%94logistic%E5%9B%9E%E5%BD%92/2.png" alt="img" style="zoom:100%;"></p><h3 id="2-假设函数"><a href="#2-假设函数" class="headerlink" title="2 假设函数"></a><strong>2 假设函数</strong></h3><p>&emsp;&emsp;在线性回归中，其假设函数为$h_\theta(x)=\theta^Tx$，但是在分类算法中，使用线性回归不是一个好的方法，因为线性回归中，其分类曲线被样本值的影响较大。比如，状态为1的样本在远离状态为0的样本簇的方向多了n个，可能会导致线性回归曲线向状态为1的样本处偏移。而实际上，状态之间的分类界限并未受到变化，其分类曲线不应当受到这类样本值的影响。因此，使用线性回归的假设不能够正确求解logistic回归，因此引入了sigmoid函数，logistic回归的假设函数即基于sigmoid函数提出。</p><p>&emsp;&emsp;sigmoid函数的图像如下图所示：</p><p><img src="/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(2)%E2%80%94%E2%80%94logistic%E5%9B%9E%E5%BD%92/1.png" alt="img" style="zoom:180%;"></p><p>&emsp;&emsp;可以看出，sigmoid函数（也被称作为激活函数）$g(z)=\frac{1}{1+e^{-z}}$ 具有比较明显的特点，其值在$z&gt;0$处基本上可以认为$g(z)$为1，而在$z&lt;0$处，基本上可以认为$g(z)$为0。从另一层面上来说，如果$g(z)\ge0.5$，则可以认为$g(z)$为1；反之$g(z)&lt;0.5$，则可以认为$g(z)$为0。</p><p>&emsp;&emsp;sigmoid函数的这一特性恰好方便了其去构造分类假设函数。假设函数应当为$g(z)$的复合函数，从而可以根据假设函数的值来对状态进行分类预测。</p><p>&emsp;&emsp;事实上，假设函数被定义为如下：</p><script type="math/tex; mode=display">h_{\theta}\left( x \right) =g\left( \theta ^Tx \right) =\frac{1}{1+e^{-\theta ^Tx}}</script><p>&emsp;&emsp;粗略地说，假设函数也有另一层含义，对于$h_\theta(x)=0.7$，它可以代表“当$x$值给定时，对于一个参数$\theta$，其$y=1$的概率，它又有下面概率意义上的关系式：</p><script type="math/tex; mode=display">P\left( y=0|x,\theta \right) +P\left( y=1|x,\theta \right) =1</script><h3 id="3-代价函数"><a href="#3-代价函数" class="headerlink" title="3 代价函数"></a><strong>3 代价函数</strong></h3><p>&emsp;&emsp;在线性回归中，代价函数为$J\left( \theta \right) =\frac{1}{2m}\sum<em>{i=1}^m{\left( h</em>{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}$。然而，在logistic回归中，由于$h_\theta(x)$较为复杂，如果仍使用线性回归中的定义方式，可能会得到下图A中的函数图像，可以看出，它不是凸函数，不好根据代价函数对参数$\theta$进行后续求解工作。我们想要研究的是B类的函数图像，因此，logistic回归中，代价函数需要重新定义。</p><p><img src="/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(2)%E2%80%94%E2%80%94logistic%E5%9B%9E%E5%BD%92/3.png" alt="img" style="zoom:150%;"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> logistic回归 </tag>
            
            <tag> 分类 </tag>
            
            <tag> sigmoid函数 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
